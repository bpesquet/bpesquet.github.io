<!doctype html><html lang=en data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>Confidence in decision-making - Baptiste Pesquet</title><meta name=description content="Confidence in decision-making Terminology Confidence: general definition Everyone knows intuitively what confidence is about, yet it is seldom defined explicitely.
In the broadest sense, confidence quantifies a degree of belief in something or someone [Meyniel et al., 2015].
It is fundamentally linked to its object: a thought, a choice, an external actor, etc.
Belief A belief is a feeling of certainty about a proposition (i.e. a statement or a decision)."><link rel=icon type=image/x-icon href=https://www.bpesquet.fr/favicon.ico><link rel=apple-touch-icon-precomposed href=https://www.bpesquet.fr/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=https://www.bpesquet.fr/css/style.min.ecd6d68ec75f136aa2ef44e76818d4e46cf4ee45f39dae3684c8b4232785015c.css integrity="sha256-7NbWjsdfE2qi70TnaBjU5Gz07kXzna42hMi0IyeFAVw="><script src=https://www.bpesquet.fr/js/script.min.74bf1a3fcf1af396efa4acf3e660e876b61a2153ab9cbe1893ac24ea6d4f94ee.js type=text/javascript integrity="sha256-dL8aP88a85bvpKzz5mDodrYaIVOrnL4Yk6wk6m1PlO4="></script><meta property="og:title" content="Confidence in decision-making"><meta property="og:description" content="Confidence in decision-making Terminology Confidence: general definition Everyone knows intuitively what confidence is about, yet it is seldom defined explicitely.
In the broadest sense, confidence quantifies a degree of belief in something or someone [Meyniel et al., 2015].
It is fundamentally linked to its object: a thought, a choice, an external actor, etc.
Belief A belief is a feeling of certainty about a proposition (i.e. a statement or a decision)."><meta property="og:type" content="article"><meta property="og:url" content="https://www.bpesquet.fr/presentations/confidence/"><meta property="article:section" content="presentations"><meta property="article:published_time" content="2024-06-24T10:13:06+01:00"><meta property="article:modified_time" content="2025-02-17T13:50:57+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Confidence in decision-making"><meta name=twitter:description content="Confidence in decision-making Terminology Confidence: general definition Everyone knows intuitively what confidence is about, yet it is seldom defined explicitely.
In the broadest sense, confidence quantifies a degree of belief in something or someone [Meyniel et al., 2015].
It is fundamentally linked to its object: a thought, a choice, an external actor, etc.
Belief A belief is a feeling of certainty about a proposition (i.e. a statement or a decision)."><script async src="https://www.googletagmanager.com/gtag/js?id=G-4DNVMNCRYK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-4DNVMNCRYK",{anonymize_ip:!1})}</script></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class=site-title><a href=/>Baptiste Pesquet</a></h1><ul class=social-icons><li><a href="mailto:bpesquet [at] gmail [dot] com" title=Email rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M464 64H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 4e2V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V4e2H48z"/></svg></span></a></li><li><a href=https://github.com/bpesquet title=Github rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a></li><li><a href="https://scholar.google.fr/citations?hl=fr&amp;user=0KJ7JkMAAAAJ" title="Google scholar" rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64h0c1.7-3.6 3.6-7.2 5.6-10.7 4.4-7.6 9.4-14.7 15-21.3 27.4-32.6 68.5-53.3 114.4-53.3 33.6.0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0 512 202.7l-94.7 77.1z"/></svg></span></a></li><li><a href=https://www.linkedin.com/in/bpesquet title=Linkedin rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1.0 83.5.0 53.8a53.8 53.8.0 01107.6.0c0 29.7-24.1 54.3-53.8 54.3zM447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3.0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a></li><li><a href=https://bsky.app/profile/bpesquet.bsky.social title=Bluesky rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentcolor" d="M123.6 34.5C190 84.6 261.5 186 287.8 240.5 314 186 385.5 84.5 452 34.5c48-36.1 125.6-64.1 125.6 24.9.0 17.8-10.1 149.2-16.1 170.5-20.7 74.2-96.1 93.1-163.1 81.6 117.2 20 147 86.3 82.6 152.6-122.3 125.9-175.8-31.6-189.5-72-2.5-7.5-3.7-10.9-3.7-7.9.0-3.1-1.2.4-3.7 7.9-13.7 40.4-67.2 197.9-189.5 72C30.2 397.8 60 331.5 177.2 311.5c-67 11.4-142.4-7.5-163.1-81.7C8.1 208.5-2 77.1-2 59.3c0-88.9 77.7-61 125.6-24.9z"/></svg></span></a></li><li><a href=https://www.youtube.com/c/../@bpesquet title=Youtube rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentcolor" d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78.0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg></span></a></li></ul></div><div class=header-top-right><div class=theme-switcher><span class=inline-svg><svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 290 290"><path fill="currentcolor" d="M142.959.0C64.131.0.0 64.132.0 142.96s64.131 142.959 142.959 142.959 142.96-64.131 142.96-142.959C285.919 64.132 221.787.0 142.959.0zm0 260.919V142.96 25c65.043.0 117.96 52.917 117.96 117.96.0 65.043-52.917 117.959-117.96 117.959z"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme)};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme=="dark"&&document.documentElement.setAttribute("data-theme","dark"),currentTheme=="auto"&&(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":"light"}function switchTheme(){currentTheme=="dark"?(localStorage!==null&&localStorage.setItem(STORAGE_KEY,"light"),document.documentElement.setAttribute("data-theme","light"),currentTheme="light"):(localStorage!==null&&localStorage.setItem(STORAGE_KEY,"dark"),document.documentElement.setAttribute("data-theme","dark"),currentTheme="dark")}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}</script></div></div><nav><a href=https://www.bpesquet.fr/bio/ title="Short bio">Short bio</a>
<a href=https://www.bpesquet.fr/teaching/ title=Teaching>Teaching</a>
<a href=https://www.bpesquet.fr/research/ title=Research>Research</a>
<a href=https://www.bpesquet.fr/talks/ title="Talks and presentations">Talks</a>
<a href=https://www.bpesquet.fr/posts/ title="Blog archive">Blog</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">Confidence in decision-making</h1></header></div><nav id=TableOfContents><ul><li><a href=#terminology>Terminology</a><ul><li><a href=#confidence-general-definition>Confidence: general definition</a></li><li><a href=#belief>Belief</a></li><li><a href=#uncertainty>Uncertainty</a></li><li><a href=#confidence-updated-definition>Confidence: updated definition</a></li><li><a href=#trust>Trust</a></li><li><a href=#error-monitoring>Error monitoring</a></li><li><a href=#metacognition>Metacognition</a></li><li><a href=#cognitive-control>Cognitive control</a></li></ul></li><li><a href=#measuring-confidence>Measuring confidence</a><ul><li><a href=#experimental-tasks>Experimental tasks</a></li><li><a href=#measures-of-interest>Measures of interest</a></li><li><a href=#measurement-methods>Measurement methods</a></li></ul></li><li><a href=#computing-confidence>Computing confidence</a><ul><li><a href=#context>Context</a></li><li><a href=#statistical-correlation>Statistical correlation</a></li><li><a href=#signal-detection-theory>Signal Detection Theory</a></li><li><a href=#evidence-accumulation-models>Evidence Accumulation Models</a></li></ul></li><li><a href=#whats-next>What&rsquo;s next?</a><ul><li><a href=#neural-basis-of-confidence>Neural basis of confidence</a></li><li><a href=#usages-of-confidence>Usages of confidence</a></li></ul></li></ul></nav><div class="content e-content"><h1 id=confidence-in-decision-making>Confidence in decision-making
<span><a href=#confidence-in-decision-making><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><h2 id=terminology>Terminology
<span><a href=#terminology><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h2><h3 id=confidence-general-definition>Confidence: general definition
<span><a href=#confidence-general-definition><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Everyone knows intuitively what confidence is about, yet it is seldom defined explicitely.</p><p>In the broadest sense, <strong>confidence quantifies a degree of belief in something or someone</strong> <a href=https://doi.org/10.1016/j.neuron.2015.09.039>[Meyniel et al., 2015]</a>.</p><p>It is fundamentally linked to its object: a thought, a choice, an external actor, etc.</p><h3 id=belief>Belief
<span><a href=#belief><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>A belief is <strong>a feeling of certainty about a proposition</strong> (i.e. a statement or a decision). It is a subjective, conscious experience.</p><p>Note: regarding perceptions, our belief usually matches our perceptual experience, but not always <a href=https://journals.sagepub.com/doi/10.1177/0301006620928010>[Mamassian, 2020]</a>.</p><p><img src=images/belief_perception_gap.png alt="Belief-perception gap"></p><h3 id=uncertainty>Uncertainty
<span><a href=#uncertainty><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Generally speaking, uncertainty (or incertitude) characterizes situations involving <strong>imperfect, noisy or unknown information</strong>.</p><p>In decision-making, uncertainty refers to <strong>the variability in the representation of information before a decision is taken</strong> <a href=https://journals.sagepub.com/doi/10.1177/0301006620928010>[Mamassian, 2021]</a>.</p><p>To perform well, the brain needs to be effective at dealing with many uncertainties, some of them external (changes in world state or sensorimotor variability), others internal (cognitive variables, timing or abstract states). Uncertainty is inherent to all stages of neural computation.</p><h3 id=confidence-updated-definition>Confidence: updated definition
<span><a href=#confidence-updated-definition><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>In decision-making, confidence can be seen as <strong>the subjective estimate of decision quality</strong> <a href=https://www.nature.com/articles/s41467-021-27618-5>[Brus et al., 2021]</a>.</p><p>More formally, it can be defined as <strong>the probability that a choice is correct given the evidence</strong> <a href=https://www.nature.com/articles/nn.4240>[Pouget et al., 2016]</a>.</p><p>Confidence is a form of certainty. A key difference is that contrary to confidence, (un)certainties are <em>decision independant</em>. <strong>Confidence quantifies the degree of certainty associated to a decision</strong>.</p><h3 id=trust>Trust
<span><a href=#trust><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Trust is a social construct: <strong>the belief that someone or something will behave or perform as expected</strong>. It implies a relationship between a <em>trustor</em> and a <em>trustee</em>.</p><p><strong>Self-confidence</strong> is trust in one&rsquo;s abilities.</p><h3 id=error-monitoring>Error monitoring
<span><a href=#error-monitoring><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>In decision-making, error monitoring (EM) is <strong>the process by which one is able to detect his/her errors as soon as a response has been made</strong> <a href=https://royalsocietypublishing.org/doi/10.1098/rstb.2011.0416>[Yeung & Summerfield, 2012]</a>.</p><p>EM allows adaptation of behavior both in the short and longer terms through gradual learning of actions&rsquo; outcomes.</p><h3 id=metacognition>Metacognition
<span><a href=#metacognition><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Confidence judgments and error monitoring are two related aspects of metacognition (sometimes called <em>higher order thinking</em>).</p><p>First described in <a href=https://www.semanticscholar.org/paper/Metacognition-and-Cognitive-Monitoring%3A-A-New-Area-Flavell/ee652f0f63ed5b0cfe0af4cb4ea76b2ecf790c8d>[Flavell, 1979]</a>, metacognition can be defined as <strong>the ability to consider, understand and regulate one&rsquo;s cognitive processes</strong>. It is a key skill to adapt to complex problems and changing environments.</p><hr><p>Metacognition is classicaly divided into two subprocesses: <strong>monitoring</strong> and <strong>control</strong>.</p><p><img src=images/metacognition_diagram.png alt="Metacognition diagram"></p><h4 id=example-of-metacognition-metaperception>Example of metacognition: metaperception
<span><a href=#example-of-metacognition-metaperception><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p><img src=images/metaperception.png alt=Metaperception></p><p><a href=https://journals.sagepub.com/doi/10.1177/0301006620928010>[Mamassian, 2020]</a></p><h3 id=cognitive-control>Cognitive control
<span><a href=#cognitive-control><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Cognitive control refers to <strong>the intentional selection of thoughts, emotions, and behaviors based on current task demands and social context, and the concomitant suppression of inappropriate habitual actions</strong> <a href=https://www.annualreviews.org/content/journals/10.1146/annurev.neuro.24.1.167>[Miller and Cohen, 2001]</a>.</p><p>In simpler terms, cognitive control allows adapting our behaviour on-the-fly to improve performance.</p><h2 id=measuring-confidence>Measuring confidence
<span><a href=#measuring-confidence><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h2><h3 id=experimental-tasks>Experimental tasks
<span><a href=#experimental-tasks><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Their setup is similar to those used to study <a href=../decision_making/README.md>decision-making</a>. The major difference is that before or (more frequently) after taking a decision (a <em>type 1</em> task), subjects express their confidence about it (a <em>type 2</em> task).</p><p>Example of type 1 task: is the <a href=http://neuroanatody.com/2016/05/whats-in-a-gabor-patch/>Gabor patch</a> tilted to the left or to the right?</p><p><img src=images/gabor_patch.png alt="Gabor patch"></p><h4 id=flow-of-information-for-a-perceptual-task>Flow of information for a perceptual task
<span><a href=#flow-of-information-for-a-perceptual-task><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p><img src=images/confidence_information_flow.png alt="Flow of information for a perceptual task"></p><p><a href=https://journals.sagepub.com/doi/10.1177/0301006620928010>[Mamassian, 2020]</a></p><h3 id=measures-of-interest>Measures of interest
<span><a href=#measures-of-interest><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Measures of metacognition in experimental tasks seek to estimate the statistical relationship between confidence judgments and objective performance <a href=https://www.annualreviews.org/content/journals/10.1146/annurev-psych-022423-032425>[Fleming, 2024]</a>.</p><h4 id=sensitivity>Sensitivity
<span><a href=#sensitivity><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>Confidence/metacognitive/type 2 sensitivity is <strong>the capacity to correlate confidence judgments and objective task performance</strong>.</p><p>For example, being confident when taking correct decisions and less confident otherwise demonstrates a high degree of sensitivity.</p><p>Sensitivity is often affected by task performance itself: an individual will appear to have greater sensitivity on an easy task compared to a hard task <a href=https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00443/full>[Fleming and Lau, 2014]</a>.</p><h4 id=bias>Bias
<span><a href=#bias><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>Confidence/metacognitive/type 2 bias is <strong>a difference in subjective confidence despite constant task performance</strong>.</p><p>Under- and over-confidence are examples of biases.</p><p><img src=images/sensitivity_vs_bias.png alt="Sensitivity Vs bias"></p><blockquote><p>Real confidence distributions are unlikely to be Gaussian.</p></blockquote><p><a href=https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00443/full>[Fleming and Lau, 2014]</a></p><h4 id=efficiency>Efficiency
<span><a href=#efficiency><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>Confidence/metacognitive efficiency (or capacity) is <strong>the level of sensitivity given a certain level of task performance</strong>.</p><p>It is measured relative to a particular task performance level.</p><h3 id=measurement-methods>Measurement methods
<span><a href=#measurement-methods><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><h4 id=confidence-ratings>Confidence ratings
<span><a href=#confidence-ratings><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p><img src=images/confidence_ratings.png alt="Confidence ratings"></p><p>After a decision, the subject is asked to evaluate its correctness, using a dedicated scale.</p><p>Simple and frequently used, this method has several drawbacks: intersubject variability regarding bias and scale usage, and possible confusions between type 1 and type 2 judgments <a href=https://journals.sagepub.com/doi/10.1177/0301006620928010>[Mamassian, 2020]</a>.</p><h4 id=post-decision-wagering>Post-decision wagering
<span><a href=#post-decision-wagering><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>After a decision, subjects are asked to gamble on whether their response was correct. If the decision is correct, the wager amount is kept <a href=https://royalsocietypublishing.org/doi/10.1098/rstb.2011.0417>[Fleming and Dolan, 2012]</a>.</p><p>The amount of the bet is assumed to reflect a subject’s confidence in his or her decision.</p><h4 id=opt-out-paradigm>Opt-out paradigm
<span><a href=#opt-out-paradigm><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p><img src=images/confidence_opt_out.png alt="Confidence opt-out"></p><p>In most but not all the trials, the subject has the option to decline the decision task and receive a smaller reward.</p><p>This paradigm is well suited to experiments with animals, which cannot explicitely report their confidence.</p><p>One challenge is to avoid confounding it with a three-alternative forced choice <a href=https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0037>[Kepecs et al., 2912]</a>.</p><h4 id=confidence-forced-choice>Confidence forced choice
<span><a href=#confidence-forced-choice><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p><img src=images/confidence_forced_choice.png alt="Confidence forced choice"></p><p>After two decisions, the subject has to choose which one is more likely to be correct.</p><p>One benefit of this paradigm is that it disregards confidence biases to focus on sensitivity <a href=https://journals.sagepub.com/doi/10.1177/0301006620928010>[Mamassian, 2020]</a>.</p><h2 id=computing-confidence>Computing confidence
<span><a href=#computing-confidence><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h2><h3 id=context>Context
<span><a href=#context><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Let&rsquo;s consider a simple two-alternative forced choice decision task.</p><p>Assuming that post-decisional confidence was measured on a binary scale (high/low), we can count the number of confidence ratings assigned to each judgment in the following type 2 table.</p><table><thead><tr><th>Type 1 decision</th><th>Low confidence</th><th>High confidence</th></tr></thead><tbody><tr><td>Incorrect</td><td>True Negatives ($TN_2$)</td><td>False Positives ($FP_2$)</td></tr><tr><td>Correct</td><td>False Negatives ($FN_2$)</td><td>True Positives ($TP_2$)</td></tr></tbody></table><h3 id=statistical-correlation>Statistical correlation
<span><a href=#statistical-correlation><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>The simplest measure of confidence sensitivity is the <a href=https://en.wikipedia.org/wiki/Phi_coefficient>$\phi$ coefficient</a> (a.k.a. Pearson $r$ correlation for binary variables) between task performance and confidence measurements.</p><p>$$\phi = \frac{(TN_2 <em>TP_2 - FN_2</em>FP_2)}{\sqrt{(TP_2+FP_2)(TP_2+FN_2)(TN_2+FP_2)(TN_2+FN_2)}}$$</p><blockquote><p>This metric is equivalent to the <em>Matthews Correlation Coefficient</em> (MCC) used in Machine Learning <a href=https://biodatamining.biomedcentral.com/articles/10.1186/s13040-023-00322-4>[Chicco and Jurman, 2023]</a>.</p></blockquote><p>Another possible way of computing correlation is the <em>Goodman-Kruskal gamma coefficient</em> $G$.</p><p>Unfortunately, both $\phi$ and $G$ can be affected by bias.</p><h3 id=signal-detection-theory>Signal Detection Theory
<span><a href=#signal-detection-theory><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>SDT is a framework for analyzing decision making in the presence of uncertainty.</p><p>Originally developped in the mid-20th century to assess how faithfully a radar operator is able to separate signal from noise, it has applications in many fields (psychology, diagnostics, quality control, etc).</p><p>SDT&rsquo;s main virtue is its ability to disentangle sensitivity from bias in a decision process.</p><h4 id=conceptual-overview>Conceptual overview
<span><a href=#conceptual-overview><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><h5 id=context-1>Context
<span><a href=#context-1><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h5><p>In an experiment where stimuli or signals were either present or absent, and the subject categorized each trial as having the stimulus/signal present or absent, the trials are sorted into one of four categories in the following type 1 table.</p><table><thead><tr><th>Stimulus or signal</th><th>Response: &ldquo;absent&rdquo;</th><th>Response: &ldquo;present&rdquo;</th></tr></thead><tbody><tr><td>Absent</td><td>Correct Rejections ($TN_1$)</td><td>False Alarms ($FP_1$)</td></tr><tr><td>Present</td><td>Misses ($FN_1$)</td><td>Hits ($TP_1$)</td></tr></tbody></table><h5 id=discrimination-metrics>Discrimination metrics
<span><a href=#discrimination-metrics><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h5><p><em>True Positive Rate (TPR)</em> a.k.a. <em>hit rate</em> is the proportion of hits in the presence of stimulus/signal. It quantifies how well a decision maker can identify true positives.</p><p><em>False Positive Rate (FPR)</em> a.k.a. <em>false alarm rate</em> is the proportion of false alarms in the absence of stimulus/signal.</p><p>$$\text{TPR}_1 = \frac{TP_1}{TP_1 + FN_1}$$</p><p>$$\text{FPR}_1 = \frac{FP_1}{TN_1+FP_1}$$</p><blockquote><p>$\text{TPR}_1$ is equivalent to the <em>recall</em> metric used in Machine Learning.</p></blockquote><h5 id=probability-distributions>Probability distributions
<span><a href=#probability-distributions><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h5><p>SDT represents a decision as a comparison between a <em>decision variable</em> (DV), derived from a single piece of sensory evidence, and a <em>criterion</em> (the threshold between &ldquo;absent&rdquo; and &ldquo;present&rdquo; responses).</p><p>Since evidence is affected by perturbations such as neural noise and fluctuation in attention, the DV can be modelized as a random variable described by a probability distribution.</p><p>More precisely, SDT assumes that the distributions of DV values in the presence or absence of stimulus/signal are Gaussian with equal variance.</p><hr><p><img src=images/sdt_standard.png alt="Standard model of SDT"></p><p><a href=https://wires.onlinelibrary.wiley.com/doi/10.1002/wcs.1628>[Michel, 2023]</a></p><h5 id=example>Example
<span><a href=#example><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h5><p><img src=images/sdt_example.png alt="Example of criterion choice"></p><p>With this criterion choice, the TPR (shaded region of the signal distribution) is 0.9332 and the FPR (shaded region of the noise distribution) is 0.3085 <a href=https://link.springer.com/article/10.3758/BF03207704>[Stanislaw and Todorov, 1999]</a>.</p><h4 id=type-1-sensitivity-index>Type 1 sensitivity index
<span><a href=#type-1-sensitivity-index><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>Type 1 sensitivity/discriminability index $d&rsquo;_1$ is a measure of discrimination performance in the task. It quantifies the sensibility of the decision maker to the presence of the stimulus/signal.</p><p>$d&rsquo;_1$ quantifies the distance between the means of the signal and noise distributions in standard deviation units. It can be obtained using the inverse cumulative distribution function, which computes the <em>standard score</em> a.k.a. <em>z-score</em> associated to a probability <a href=https://link.springer.com/article/10.3758/BF03207704>[Stanislaw and Todorov, 1999]</a>.</p><p>$$d&rsquo;_1 = z(\text{TPR}_1) - z(\text{FPR}_1)$$</p><h4 id=roc-curve-and-auroc>ROC curve and AUROC
<span><a href=#roc-curve-and-auroc><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>The ROC curve (&ldquo;Receiver Operating Characteristic&rdquo;) plots TPR vs. FPR for each possible value of the decision criterion.</p><p>AUC, or more precisely AUROC (&ldquo;Area Under the ROC Curve&rdquo;), provides an aggregate measure of performance across all possible decision criterions. It is a way to assess sensitivity independently of bias.</p><p>This non-parametric approach is free from the equal-variance Gaussian assumption needed for $d&rsquo;$ to be bias-free <a href=https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00443/full>[Fleming and Lau, 2014]</a>.</p><h5 id=examples>Examples
<span><a href=#examples><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h5><p><img src=images/sdt_roc_examples.png alt="Examples of ROC curves"></p><p><a href=https://wires.onlinelibrary.wiley.com/doi/10.1002/wcs.1628>[Michel, 2023]</a></p><h5 id=impact-of-criterion-choice>Impact of criterion choice
<span><a href=#impact-of-criterion-choice><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h5><p><a href=https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation><img src=images/auroc_animation.gif alt="AUROC animation"></a></p><h5 id=impact-of-signal-discriminability>Impact of signal discriminability
<span><a href=#impact-of-signal-discriminability><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h5><p><a href=https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation><img src=images/auroc_shape_animation.gif alt="AUROC shape animation"></a></p><h4 id=type-2-sensitivity-index>Type 2 sensitivity index
<span><a href=#type-2-sensitivity-index><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>Applying SDT to the type 2 confidence table defined above, we can compute type 2 sensitivity $d&rsquo;_2$ by applying the same formula, using True and False Positive Rates that link accuracy and confidence.</p><p>$$d&rsquo;_2 = z(\text{TPR}_2) - z(\text{FPR}_2)$$</p><p>However, the equal-variance Gaussian assumption for distributions is problematic in this case <a href=https://link.springer.com/article/10.3758/BF03196546>[Galvin et al., 2003]</a>.</p><h4 id=auroc2>AUROC2
<span><a href=#auroc2><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>With multiple confidence ratings, it is possible to construct a type 2 ROC curve by treating each confidence level as a criterion that separates high from low confidence. AUROC2 is then a (theorically) bias-free measure of confidence sensitivity.</p><p><img src=images/auroc2.png alt="Example of ROC curve for confidence"></p><p><a href=https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00443/full>[Fleming and Lau, 2014]</a></p><h4 id=meta-d>Meta-d'
<span><a href=#meta-d><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>This measure exploits the fact that given Gaussian variance assumptions at the type 1 level, the shapes of the type 2 distributions are known even if they are not themselves Gaussian. More precisely, the type 2 ROC curve is entirely determined by type 1 sensitivity if the subject is metacognitively ideal (perfect in placing their confidence ratings).</p><p>Using this assumption and given the subject’s type 2 performance data, we can thus obtain the underlying type 1 sensitivity. This measure is called meta-$d&rsquo;$. It estimates the level of type 1 performance ($d′_1$) that would have given rise to the observed type 2 data <a href=https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00443/full>[Fleming and Lau, 2014]</a> (<a href=https://www.columbia.edu/~bsm2105/type2sdt/>more details</a>).</p><h4 id=m-ratio>M-ratio
<span><a href=#m-ratio><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>Because meta-$d′$ is in the same units as (type 1) $d′$, the two can be directly compared.</p><p>We can define confidence efficiency as the value of meta-$d′$ relative to $d′$, or meta-$d&rsquo;/d&rsquo;$. This measure is called the M-ratio.</p><p>An alternative measure is meta-$d&rsquo;-d&rsquo;$, favored when $d&rsquo;$ takes small values <a href=https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00443/full>[Fleming and Lau, 2014]</a>.</p><h3 id=evidence-accumulation-models>Evidence Accumulation Models
<span><a href=#evidence-accumulation-models><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>In contrast to models inspired by SDT (which is silent on decision time), accumulation of evidence models assume that new sensory evidence becomes available over time until a decision is reached.</p><p>The number of accumulators may vary from only one (à la Drift Diffusion Model) to several ones, more or less partially correlated (for example, using mutual inhibition).</p><h4 id=balance-of-evidence>Balance of Evidence
<span><a href=#balance-of-evidence><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>In a multi-accumulator model, confidence can be seen as the distance between them at the time of decision (i.e. threshold reached). This measure is called the <em>Balance of Evidence</em> (BoE) <a href=https://www.annualreviews.org/content/journals/10.1146/annurev-vision-111815-114630>[Mamassian, 2016]</a>.</p><p><img src=images/BoE.png alt="Balance of Evidence example"></p><p>In a DDM-like model, confidence is taken to be the current position of the accumulated evidence.</p><h4 id=two-stage-dynamic-signal-detection>Two-stage Dynamic Signal Detection
<span><a href=#two-stage-dynamic-signal-detection><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h4><p>Other approaches like the <em>Two-stage Dynamic Signal Detection</em> (2DSD) model postulate that the accumulation process continues after a decision has been made. The ultimate location of accumulated evidence serves as a proxy for confidence.</p><p>These approaches of confidence formation may help explain some experimentaly reported phenomena like post-decisional changes of mind.</p><p><img src=images/2DSD.png alt="2DSD model"></p><p><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0019737">[Pleskac and Busemeyer, 2010]</a></p><h2 id=whats-next>What&rsquo;s next?
<span><a href=#whats-next><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h2><h3 id=neural-basis-of-confidence>Neural basis of confidence
<span><a href=#neural-basis-of-confidence><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>Numerous studies demonstrate that the brain tracks uncertainty about a wide range of quantities and that such uncertainty informs metacognitive processes, such as confidence judgments <a href=https://www.annualreviews.org/content/journals/10.1146/annurev-psych-022423-032425>[Fleming, 2024]</a>.</p><p>Activity in the parietal cortex seems related to evidence accumulation during decision-making.</p><p>Convergent findings emphasize the importance of the prefrontal cortex, more precisely the ventromedial prefrontal cortex (vmPFC), in the formation of confidence.</p><p>Many results suggest that there are separate and perhaps multiple brain areas involved in confidence monitoring and reporting <a href=https://www.sciencedirect.com/science/article/pii/S0149763415001025?via%3Dihub>[Grimaldi et al., 2015]</a>.</p><h3 id=usages-of-confidence>Usages of confidence
<span><a href=#usages-of-confidence><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>As part of metacognitive monitoring, confidence judgments may inform the processes of cognitive control.</p><p>Having an explicit representation of the confidence of a perceptual decision may help us compute the risk of being wrong ($1 - \text{confidence}$).</p><p>Having a good confidence sensitivity will also give us the possibility to allocate appropriate resources to a task.</p><p>Good confidence can also help us appreciate whether and how we can control the environment.</p></div><div class=post-info><div class="post-date dt-published"><a class=u-url href=/presentations/confidence/><time datetime=2024-06-24>June 24, 2024</time></a>
[Last modified: <time datetime=2025-02-17>February 17, 2025</time>]</div><a class="post-hidden-url u-url" href=https://www.bpesquet.fr/presentations/confidence/>https://www.bpesquet.fr/presentations/confidence/</a>
<a href=https://www.bpesquet.fr class="p-name p-author post-hidden-author h-card" rel=me>Baptiste Pesquet</a><div class=post-taxonomies><ul class=post-tags><li><a href=https://www.bpesquet.fr/tags/decision-making/>#decision-making</a></li><li><a href=https://www.bpesquet.fr/tags/confidence/>#confidence</a></li></ul></div></div></article><div class="pagination post-pagination"><div class="left pagination-item"><a href=/presentations/mnemosyne-team-meeting-2024/>Confidence as hyperparameter tuning for sequential decision-making</a></div><div class="right pagination-item"><a href=/presentations/decision-making/>Introduction to the theory of decision-making</a></div></div></main><footer class=common-footer><div class=common-footer-bottom><div class=copyright><p>© Baptiste Pesquet, 2025<br>Powered by <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, theme <a target=_blank rel="noopener noreferrer" href=https://github.com/mitrichius/hugo-theme-anubis>Anubis</a>.<br></p></div></div><p class="h-card vcard"><a href=https://www.bpesquet.fr class="p-name u-url url fn" rel=me>Baptiste Pesquet</a>
/
<a class="p-email u-email email" rel=me href=mailto:bpesquet%20[at]%20gmail%20[dot]%20com>bpesquet [at] gmail [dot] com</a></p></footer></div></body></html>